{"pubDate": "2024-02-26T12:00:41", "original_title": "On-click install local AI applications using pinokio", "link": "https://hackaday.com/2024/02/26/on-click-install-local-ai-applications-using-pinokio/", "source": "https://hackaday.com/blog/feed/", "thumbnail": "https://hackaday.com/wp-content/uploads/2024/02/pinokio5.7e022ccd.png", "original_content": "Pinokio is billed as an autonomous virtual computer, which could mean anything really, but dont click away just yet, because this is one heck of a project. AI enthusiast [cocktail peanut] (and other undisclosed contributors) has created a browser-style application which enables a virtual Unix-like environment to be embedded, regardless of the host architecture. A discover page loads up registered applications from GitHub, allowing a one-click install process, which is simply a JSON file describing the dependencies and execution flow. The idea is rather than manually running commands and satisfying dependencies, its all wrapped up for you, enabling a one-click to download and install everything needed to run the application.\nBut what applications? we hear you ask, AI ones. Lots of them. The main driver seems to be to use the Pinokio hosting environment to enable easy deployment of AI applications, directly onto your machine. One click to install the app, then another one to download models, and whatever is needed, from the likes of HuggingFace and friends. A final click to launch the app, and a browser window opens, giving you a web UI to control the locally running AI backend.\nMany chat-type models are supported, as is stable diffusion and many other fun time sinks. Running AI applications on your hardware, with your data, and privately, is a total breeze. Unless an application needs external API access, no internet connection is needed. No sign-ups and no subscription costs! There are some obvious gotchas; AI applications need a lot of resources, so you will need plenty of RAM and CPU cores to get anything working, and for the vast majority of applications, a modern GPU with plenty of VRAM. Our testing showed that some apps needed a minimum of 4GiB of VRAM to even start, but a few ran on only the CPU. It just depends. We reckon youll need at least 8GiB to run the older stable diffusion 1.5 model, but that will not come as a great shock to some of you.\nFor a bit more of an intro to how all this works, and what you can do with it, check out the docs. The project is open source, but we havent located the source yet. Perhaps more testing is being performed first? Finally, there is an active discord as well, if you get stuck.\nAI is not news here, heres a little something allowing you to chat with a locally hosted LLM.\u00a0 If this project isnt self-contained enough for you, why not check out the AI in A Box?\n"}