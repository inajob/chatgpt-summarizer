{"pubDate": "2025-04-12T20:00:42", "original_title": "Vibe Check: False Packages a New LLM Security Risk?", "link": "https://hackaday.com/2025/04/12/vibe-check-false-packages-a-new-llm-security-risk/", "source": "https://hackaday.com/blog/feed/", "thumbnail": "https://hackaday.com/wp-content/uploads/2025/04/LLM_halluciation_exploit_HaD.png", "original_content": "Lots of people swear by large-language model (LLM) AIs for writing code. Lots of people swear at them. Still others may be planning to exploit their peculiarities, according to [Joe Spracklen] and other researchers at USTA. At least, the researchers have found a potential exploit in vibe coding.\nEveryone who has used an LLM knows they have a propensity to hallucinate that is, to go off the rails and create plausible-sounding gibberish. When youre vibe coding, that gibberish is likely to make it into your program. Normally, that just means errors. If you are working in an environment that uses a package manager, however (like npm in Node.js, or PiPy in Python, CRAN in R-studio) that plausible-sounding nonsense code may end up calling for a fake package.\nA clever attacker might be able to determine what sort of false packages the LLM is hallucinating, and inject them as a vector for malicious code. Its more likely than you think while CodeLlama was the worst offender, the most accurate model tested (ChatGPT4) still generated these false packages at a rate of over 5%. The researchers were able to come up with a number of mitigation strategies in their full paper, but this is a sobering reminder that an AI cannot take responsibility. Ultimately it is up to us, the programmers, to ensure the integrity and security of our code, and of the libraries we include in it.\nWe just had a rollicking discussion of vibe coding, which some of you seemed quite taken with. Others agreed that ChatGPT is the worst summer intern ever.\u00a0 Love it or hate it, its likely this wont be the last time we hear of security concerns brought up by this new method of programming.\nSpecial thanks to [Wolfgang Friedrich] for sending this into our tip line."}