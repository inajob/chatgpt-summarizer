{"pubDate": "2025-07-06T11:00:00", "original_title": "Going to the (Parallel) Chapel", "link": "https://hackaday.com/2025/07/06/going-to-the-parallel-chapel/", "source": "https://hackaday.com/blog/feed/", "thumbnail": "https://hackaday.com/wp-content/uploads/2025/07/chap.png", "original_content": "There is always the promise of using more computing power for a single task. Your computer has multiple CPUs now, surely. Your video card has even more. Your computer is probably networked to a slew of other computers. But how do you write software to take advantage of that? There are many complex systems, of course, but theres also Chapel.\nChapel is a reasonably simple programming language, but it supports parallelism in various forms. The run time controls how computers  whatever that means  communicate with one another. You can have code running on your local CPUs, your GPU, and other processing elements over the network without much work on your part.\n\nWhats it look like? Heres a simple distributed program from the projects homepage:\n\n\n// print a message per compute node\ncoforall loc in Locales do\non loc do\nwriteln(quot;Hello from locale quot;, loc.id);\n\n// print a message per core per compute node\ncoforall loc in Locales do\non loc do\ncoforall tid in 0..amp;lt;here.maxTaskPar do\nwriteln(quot;Hello from task quot;, tid, quot; on locale quot;, loc.id);\n\n\nAs you might guess, Locales is an array of locale objects that each describe some computing resource. The coforall statement splits a loop up to run on different locales or CPUs. You can even write GPU kernels:\n\n\ncoforall (gpu, row) in zip(here.gpus, localRowStart..) do on gpu {\n\n\nYou can try it in your browser, but for best results, you really want to download it or run it in a container. The license is Apache 2.0, so you can even contribute if you want to. If you want to really do distributed work, be sure to grab the package built for GASNet or Slurm.\nWhile it is something new to learn, you might find it easier and more generally applicable than something like CUDA."}