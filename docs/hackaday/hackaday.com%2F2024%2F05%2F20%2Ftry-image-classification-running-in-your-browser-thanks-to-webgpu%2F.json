{"pubDate": "2024-05-20T11:00:04", "original_title": "Try Image Classification Running In Your Browser, Thanks to WebGPU", "link": "https://hackaday.com/2024/05/20/try-image-classification-running-in-your-browser-thanks-to-webgpu/", "source": "https://hackaday.com/blog/feed/", "thumbnail": "https://hackaday.com/wp-content/uploads/2024/05/WebGPU-CLIP.png", "original_content": "When something does zero-shot image classification, that means its able to make judgments about the contents of an image without the user needing to train the system beforehand on what to look for. Watch it in action with this online demo, which uses WebGPU to implement CLIP (Contrastive Language\u2013Image Pre-training) running in ones browser, using the input from an attached camera.\nBy giving the program some natural language visual concept labels (such as person or cat) that fit a hypothetical template for the image content, the system will output  in real-time  its judgement on the appropriateness of such labels to what the camera sees. Again, all of this runs locally.\nIts maybe a little bit unintuitive, but whats happening in the demo is that the system is deciding which of the user-provided labels (a photo of a cat vs a photo of a bald man, for example) is most appropriate to what the camera sees. The more a particular label is judged a good fit for the image, the higher the number beside it.\nThis kind of process benefits greatly from shoveling the hard parts of the computation onto compatible graphics cards, which is exactly what WebGPU provides by allowing the browser access to a local GPU.\u00a0WebGPU is relatively recent, but weve already seen it used to run LLMs (Large Language Models) directly in the browser.\nWondering what makes GPUs so very useful for AI-type applications? Its all about their ability to work with enormous amounts of data very quickly."}