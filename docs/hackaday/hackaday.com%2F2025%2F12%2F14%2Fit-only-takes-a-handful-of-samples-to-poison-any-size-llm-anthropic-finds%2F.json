{"pubDate": "2025-12-15T03:00:06", "original_title": "It Only Takes a Handful of Samples To Poison Any Size LLM, Anthropic Finds", "link": "https://hackaday.com/2025/12/14/it-only-takes-a-handful-of-samples-to-poison-any-size-llm-anthropic-finds/", "source": "https://hackaday.com/blog/feed/", "thumbnail": "https://hackaday.com/wp-content/uploads/2025/12/Anthropic-Attack-Success-4584x2579-1-e1765123936456.webp", "original_content": "It stands to reason that if you have access to an LLMs training data, you can influence whats coming out the other end of the inscrutable AIs network. The obvious guess is that youd need some percentage of the overall input, though exactly how much that was  2%, 1%, or less  was an active research question. New research by Anthropic, the UK AI Security Institute, and the Alan Turing Institute shows it is actually a lot easier to poison the well than that.\nWere talking parts-per-million of poison for large models, because the researchers found that with just 250 carefully-crafted poison pills, they could compromise the output of any size LLM. Now, when we say poison the model, were not talking about a total hijacking, at least in this study. The specific backdoor under investigation was getting the model to produce total gibberish.\n\nThe gibberish here is triggered by a specific phrase, seeded into the poisoned training documents. One might imagine an attacker could use this as a crude form of censorship, or a form of Denial of Service Attack  say the poisoned phrase is a web address, then any queries related to that address would output gibberish. In the tests, they specifically used the word sudo, rendering the models (which ranged from 600 million to 13 billion parameters) rather useless for POSIX users. (Unless you use doas under *BSD, but if youre on BSD you probably dont need to ask an LLM for help on the command line.)\nOur question is: Is it easier to force gibberish or lies? A denial-of-service gibberish attack is one thing, but if a malicious actor could slip such a relatively small number of documents into the training data to trick users into executing unsafe code, thats something entirely worse. Weve seen discussion of data poisoning before, and that study showed it took a shockingly small amount of misinformation in the training data to ruin a medical model.\nOnce again, the old rule rears its ugly head: trust, but verify. If youre getting help from the internet, be it random humans or randomized neural-network outputs, its on you to make sure that the advice youre getting is sane.\u00a0 Even if you trust Anthropic or OpenAI to sanitize their training data, remember that even when the data isnt poisoned, there are other ways to exploit vibe coders. Perhaps this is what happened with the whole seahorse emoji fiasco."}