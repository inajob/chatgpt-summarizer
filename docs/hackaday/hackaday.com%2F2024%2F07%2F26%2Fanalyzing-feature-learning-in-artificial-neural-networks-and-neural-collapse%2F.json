{"pubDate": "2024-07-27T02:00:20", "original_title": "Analyzing Feature Learning in Artificial Neural Networks and Neural Collapse", "link": "https://hackaday.com/2024/07/26/analyzing-feature-learning-in-artificial-neural-networks-and-neural-collapse/", "source": "https://hackaday.com/blog/feed/", "thumbnail": "https://hackaday.com/wp-content/uploads/2024/03/simplicify_biases_and_spurious_features_radhakrishnan_et_al_2024.jpg", "original_content": "Artificial Neural Networks (ANNs) are commonly used for machine vision purposes, where they are tasked with object recognition. This is accomplished by taking a multi-layer network and using a training data set to configure the weights associated with each neuron. Due to the complexity of these ANNs for non-trivial data sets, its often hard to make head or tails of what the network is actually matching in a given (non-training data) input. In a March 2024 study (preprint) by [A. Radhakrishnan] and colleagues in Science an approach is provided to elucidate and diagnose this mystery somewhat, by using what they call the average gradient outer product (AGOP).\nDefined as the uncentered covariance matrix of the ANNs input-output gradients averaged over the training dataset, this property can provide information on the data sets features used for predictions. This turns out to be strongly correlated with repetitive information, such as the presence of eyes in recognizing whether lipstick is being worn and star patterns in a car and truck data set rather than anything to do with the (highly variable) vehicles. None of this was perhaps too surprising, but a number of the same researchers used the same AGOP for elucidating the mechanism behind neural collapse (NC) in ANNs.\nNC occurs when an ANN gets overtrained (overparametrized). In the preprint paper by [D. Beaglehole] et al. the AGOP is used to provide evidence for the mechanism behind NC during feature learning. Perhaps the biggest take-away from these papers is that while ANNs can be useful, theyre also incredibly complex and poorly understood. The more we learn about their properties, the more appropriately we can use them.", "title": "\"\u6a5f\u68b0\u30d3\u30b8\u30e7\u30f3\u306e\u305f\u3081\u306e\u4eba\u5de5\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u8907\u96d1\u6027\"", "body": "ANN\u306f\u6a5f\u68b0\u30d3\u30b8\u30e7\u30f3\u3067\u5e83\u304f\u4f7f\u7528\u3055\u308c\u3001\u7269\u4f53\u8a8d\u8b58\u306b\u4f7f\u7528\u3055\u308c\u308b\u3002\u8907\u96d1\u306a\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3067\u306f\u3001\u96a0\u308c\u305f\u60c5\u5831\u306f\u7406\u89e3\u3057\u306b\u304f\u304f\u3001AGOP\u3068\u547c\u3070\u308c\u308b\u30a2\u30d7\u30ed\u30fc\u30c1\u304c\u7406\u89e3\u3068\u8a3a\u65ad\u3092\u9032\u5c55\u3055\u305b\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u3068\u5831\u544a\u3055\u308c\u3066\u3044\u308b\u3002", "titles": ["\"\u6a5f\u68b0\u30d3\u30b8\u30e7\u30f3\u306e\u305f\u3081\u306e\u4eba\u5de5\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u8907\u96d1\u6027\"", "\"\u91cd\u307f\u306e\u8a2d\u5b9a\u306b\u3088\u308b\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u8a8d\u8b58\"", "\"\u5e73\u5747\u52fe\u914d\u5916\u7a4d\u306b\u3088\u308b\u8a13\u7df4\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u5206\u6790\"", "\"\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306b\u304a\u3051\u308b\u7279\u5fb4\u5b66\u7fd2\u306e\u6a5f\u69cb\"", "\"\u4eba\u5de5\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u904e\u5b66\u7fd2\u30e1\u30ab\u30cb\u30ba\u30e0\""]}