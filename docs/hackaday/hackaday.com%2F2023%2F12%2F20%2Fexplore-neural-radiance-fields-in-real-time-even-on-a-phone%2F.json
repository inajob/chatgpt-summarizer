{"pubDate": "2023-12-20T09:00:08", "original_title": "Explore Neural Radiance Fields in Real-time, Even on a Phone", "link": "https://hackaday.com/2023/12/20/explore-neural-radiance-fields-in-real-time-even-on-a-phone/", "source": "https://hackaday.com/blog/feed/", "thumbnail": "https://hackaday.com/wp-content/uploads/2023/12/SMERF_-Streamable-Memory-Efficient-Radiance-Fields-for-Real-Time-Large-Scene-Exploration-0-46-screenshot.png", "original_content": "Neural Radiance Fields (NeRF) is a method of reconstructing complex 3D scenes from sparse 2D inputs, and the field has been growing by leaps and bounds. Viewing a reconstructed scene is still nontrivial, but theres a new innovation on the block: SMERF is a browser-based method of enabling full 3D navigation of even large scenes, efficient enough to render in real time on phones and laptops.\nDont miss the gallery of demos which will run on anything from powerful desktops to smartphones. Notable is the distinct lack of blurry, cloudy, or distorted areas which tend to appear in under-observed areas of a NeRF scene (such as indoor corners and ceilings). The technical paper explains SMERFs approach in more detail.\nNeRFs as a concept first hit the scene in 2020 and the rate of advancement has been simply astounding, especially compared to demos from just last year. Watch the short video summarizing SMERF below, and marvel at how it compares to other methods, some of which are themselves only months old.\n\n"}