{"pubDate": "2025-01-08T18:00:53", "original_title": "Running AI Locally Without Spending All Day on Setup", "link": "https://hackaday.com/2025/01/08/running-ai-locally-without-spending-all-day-on-setup/", "source": "https://hackaday.com/blog/feed/", "thumbnail": "https://hackaday.com/wp-content/uploads/2024/06/LargeLanguageModel.jpg", "youtube": "https://www.youtube.com/watch?v=M9yrb1pdO0w", "original_content": "There are many AI models out there that you can play with from companies like OpenAI, Google, and a host of others. But when you use them, you get the experience they want, and you run it on their computer. There are a variety of reasons you might not like this. You may not want your data or ideas sent through someone elses computer. Maybe you want to tune and tweak in ways they arent going to let you.\nThere are many more or less open models, but setting up to run them can be quite a chore and  unless you are very patient  require a substantial-sized video card to use as a vector processor. Theres very little help for the last problem. You can farm out processing, but then you might as well use a hosted chatbot. But there are some very easy ways to load and run many AI models on Windows, Linux, or a Mac. One of the easiest weve found is Msty. The program is free for personal use and claims to be private, although if you are really paranoid, youll want to verify that yourself.\nWhat is Msty?\nTalkin about Hackaday!\nMsty is a desktop application that lets you do several things. First, it can let you chat with an AI engine either locally or remotely. It knows about many popular options and can take your keys for paid services. For local options, it can download, install, and run the engines of your choice.\nFor services or engines that it doesnt know about, you can do your own setup, which ranges from easy to moderately difficult, depending on what you are trying to do.\nOf course, if you have a local model or even most remote ones, you can use Python or some basic interface (e.g., with ollama; there are plenty of examples). However, Msty lets you have a much richer experience. You can attach files, for example. You can export the results and look back at previous chats. If you dont want them remembered, you can chat in vapor mode or delete them later.\nEach chat lives in a folder, which can have helpful prompts to kick off the chat. So, a folder might say, You are an 8th grade math teacher or whatever other instructions you want to load before engaging in chat.\n\nMultiChat\nWhat two models think about 555s\nOne of the most interesting features is the ability to chat to multiple chatbots simultaneously. Sure, if it were just switching between them, that would be little more than a gimmick. However, you can sync the chats so that each chatbot answers the same prompt, and you can easily see the differences in speed and their reply.\nFor example, I asked both Google Gemini 2.0 and Llama 3.2 how a 555 timer works, and you can see the answers were quite different.\nRAGs\nThe knowledge stack feature lets you easily grab up your own data to use as the chat source (that is RAG or Retrivial Augmented Generation) for use with certain engines. You can add files, folders, Obsidian vaults, or YouTube transcripts.\nChatting about the podcast\nFor example, I built a Knowlege Stack named Hackaday Podcast 291 using the YouTube link. I could then open a chat with Googles Gemini 2.0 beta (remotely hosted) and chat with the podcast. For example:\nYou: Who are the hosts?\ngemini-2.0-flash-exp: Elliot Williams and Al Williams are the hosts.\nYou: What kind of microscope was discussed?\ngemini-2.0-flash-exp: The text discusses a probe tip etcher that is used to make tips for a type of microscope that can image at the atomic level.\nIt would be easy to, for example, load up a bunch of PDF data sheets for a processor and, maybe, your design documents to enable discussing a particular project.\nYou can also save prompts in a library, analyze result metrics, refine prompts and results, and a host of other features. The prompt library has quite a few already available, too, ranging from an acountant to a yogi, if you dont want to define your own.\nNew Models\nThe chat features are great, and having a single interface for a host of backends is nice. However, the best feature is how the program will download, install, run, and shut down local models.\nSelecting a new local model will download and install it for use.\nTo get started, press the Local AI Model button towards the bottom of the left-hand toolbar. That will give you several choices. Be mindful that many of these are quite large, and some of them require lots of GPU memory.\nI started on a machine that had an NVidia 2060 card that had 6GB of memory. Granted, some of that is running the display. But most of it was available. Some of the smaller models would work for a bit, but eventually, Id get some strange error. That was a good enough excuse to trade up to a 12GB 3060 card, and that seems to be enough for everything Ive tried so far. Granted, some of the larger models are a little slow, but tolerably so.\nThere are more options if you press the black button at the top, or you can import GGUF models from places like huggingface. If youve already loaded models for something like ollama, you can point Msty at them. You can also point to a local server if you prefer.\nThe version I tested didnt know about the Google 2.0 model. However, when adding any of the Google models, it was easy enough to add the (free) API key and the model ID (models/gemini-2.0-flash-exp) for the new model.\nWrap Up\nYou can spend a lot of time finding and comparing different AI models. It helps to have a list, although you can wait until youve burned through the ones Msty already knows about..\nIs this the only way to run your own AI model? No, of course not. But it may well be the easiest way weve seen. Wed wish for it to be open source, but at least it is free to use for personal projects. Whats your favorite way to run AI? And, yes, we know the answer for some people is dont run AI! Thats an acceptable answer, too.", "title": "- \u72ec\u81ea\u306eAI\u30e2\u30c7\u30eb\u3092\u624b\u8efd\u306b\u5b9f\u884c\u3067\u304d\u308b\u300cMsty\u300d\u306e\u9b45\u529b", "body": "Msty\u306f\u3001\u624b\u8efd\u306bAI\u30e2\u30c7\u30eb\u3092\u5229\u7528\u3067\u304d\u308b\u30c7\u30b9\u30af\u30c8\u30c3\u30d7\u30a2\u30d7\u30ea\u3067\u3059\u3002", "titles": ["- \u72ec\u81ea\u306eAI\u30e2\u30c7\u30eb\u3092\u624b\u8efd\u306b\u5b9f\u884c\u3067\u304d\u308b\u300cMsty\u300d\u306e\u9b45\u529b", "- \u8907\u6570\u306e\u30c1\u30e3\u30c3\u30c8\u30dc\u30c3\u30c8\u3092\u540c\u6642\u306b\u6d3b\u7528\u3059\u308b\u65b0\u6a5f\u80fd\u300cMultiChat\u300d", "- \u81ea\u5206\u306e\u30c7\u30fc\u30bf\u3092\u6d3b\u7528\u3059\u308b\u300c\u77e5\u8b58\u30b9\u30bf\u30c3\u30af\u300d\u3092\u4f5c\u6210\u3059\u308b\u65b9\u6cd5", "- \u30cf\u30fc\u30c9\u30a6\u30a7\u30a2\u8981\u4ef6\u3092\u8003\u616e\u3057\u305fAI\u30e2\u30c7\u30eb\u306e\u9078\u5b9a\u306e\u30b3\u30c4", "- \u500b\u4eba\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u306b\u6700\u9069\u306a\u7121\u6599AI\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u306e\u7d39\u4ecb"]}