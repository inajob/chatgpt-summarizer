{"pubDate": "2024-07-16T20:00:25", "original_title": "Playing Rock, Paper Scissors With A Time of Flight Sensor", "link": "https://hackaday.com/2024/07/16/playing-rock-paper-scissors-with-a-time-of-flight-sensor/", "source": "https://hackaday.com/blog/feed/", "thumbnail": "https://hackaday.com/wp-content/uploads/2024/07/vlcsnap-00057_be3ac4-e1721114096679.png", "original_content": "You can do all kinds of wonderful things with cameras and image recognition. However, sometimes spatial data is useful, too. As [madmcu] demonstrates, you can use depth data from a time-of-flight sensor for gesture recognition, as seen in this rock-paper-scissors demo.\nIf youre unfamiliar with time-of-flight sensors, theyre easy enough to understand. They measure distance by determining the time it takes photons to travel from one place to another. For example, by shooting out light from the sensor and measuring how long it takes to bounce back, the sensor can determine how far away an object is. Take an array of time-of-flight measurements, and you can get simple spatial data for further analysis.\nThe build uses an Arduino Uno R4 Minima, paired with a demo board for the VL53L5CX time-of-flight sensor. The software is developed using NanoEdge AI Studio. In a basic sense, the system uses a machine learning model to classify data captured by the time-of-flight sensor into gestures matching rock, paper, or scissors\u2014or nothing, if no hand is present. If you dont find [madmcu]s tutorial enough, you can take a look at the original version from STMicroelectronics, too.\nIt takes some training, and it only works in the right lighting conditions, but this is a functional system that can determine real hand sign and play the game. Weve seen similar techniques help more advanced robots cheat at this game before, too! What a time to be alive."}