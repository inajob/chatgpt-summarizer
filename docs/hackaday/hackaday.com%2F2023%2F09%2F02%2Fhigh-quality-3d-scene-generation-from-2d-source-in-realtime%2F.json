{"pubDate": "2023-09-03T02:00:06", "original_title": "High Quality 3D Scene Generation from 2D Source, In Realtime", "link": "https://hackaday.com/2023/09/02/high-quality-3d-scene-generation-from-2d-source-in-realtime/", "source": "https://hackaday.com/blog/feed/", "thumbnail": "https://hackaday.com/wp-content/uploads/2023/08/bicycle-realtime-nerf-generation.gif", "original_content": "Heres some fascinating work presented at SIGGRAPH 2023 of a method for radiance field rendering using a novel technique called Gaussian Splatting. Whats that mean? It means synthesizing a 3D scene from 2D images, in high quality and in real time, as the short animation shown above shows.\nNeural Radiance Fields (NeRFs) are a method of leveraging machine learning to, in a way, do what photogrammetry does: synthesize complex scenes and views based on input images. But NeRFs work in a fraction of the time, and require only a fraction of the source material. There are different ways to go about this and unsurprisingly, there tends to be a clear speed vs. quality tradeoff. But as the video accompanying this new work seems to show, clever techniques mean the best of both worlds.\nA short video summary is embedded just below the page break. Interested in deeper details? The research PDF is here. The amount of development this field has seen is nothing short of staggering, and certainly higher in quality than what was state-of-the-art for NeRFs only a year ago.\n\n"}