{"pubDate": "2023-12-07T16:30:10", "original_title": "Making Visual Anagrams, With Help From Machine Learning", "link": "https://hackaday.com/2023/12/07/making-visual-anagrams-with-help-from-machine-learning/", "source": "https://hackaday.com/blog/feed/", "thumbnail": "https://hackaday.com/wp-content/uploads/2023/12/Visual-anagrams.png", "original_content": "[Daniel Geng] and others have an interesting system of generating multi-view optical illusions, or visual anagrams. Such images have more than one correct view and visual interpretation.\nWhats more, there are quite a few different methods on display: 90 degree flips and other (orthogonal) image rotations, color inversions, jigsaw permutations, and more. The project page has a generous number of examples, so go check them out!\nThe teams method uses pre-trained diffusion models \u00a0more commonly known as the secret sauce inside image-generating AIs  to evaluate and work to combine the differences between different images, and try to combine and apply it in a way that results in the model generating a good visual result. While conceptually straightforward, this process wasnt really something that could work without diffusion models driven by modern machine learning techniques.\nThe visual_anagrams GitHub repository has code\u00a0and the research paper goes into details on implementation, limitations, and gives guidance on obtaining good results. Image generation is just one of the rapidly-evolving aspects of recent innovations, and its always interesting to see unusual applications like this one."}