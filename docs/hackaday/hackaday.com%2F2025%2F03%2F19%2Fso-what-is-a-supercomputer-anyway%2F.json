{"pubDate": "2025-03-19T14:00:18", "original_title": "So What is a Supercomputer Anyway?", "link": "https://hackaday.com/2025/03/19/so-what-is-a-supercomputer-anyway/", "source": "https://hackaday.com/blog/feed/", "thumbnail": "https://hackaday.com/wp-content/uploads/2018/05/02102016_eniac_programmers_la-2e16d0ba-fill-1200x630-c0_featured.png", "original_content": "Over the decades there have been many denominations coined to classify computer systems, usually when they got used in different fields or technological improvements caused significant shifts. While the very first electronic computers were very limited and often not programmable, they would soon morph into something that wed recognize today as a computer, starting with World War 2s Colossus and ENIAC, which saw use with cryptanalysis and military weapons programs, respectively.\nThe first commercial digital electronic computer wouldnt appear until 1951, however, in the form of the Ferranti Mark 1. These 4.5 ton systems mostly found their way to universities and kin, where theyd find welcome use in engineering, architecture and scientific calculations. This became the focus of new computer systems, effectively the equivalent of a scientific calculator. Until the invention of the transistor, the idea of a computer being anything but a hulking, room-sized monstrosity was preposterous.\nA few decades later, more computer power could be crammed into less space than ever before including ever higher density storage. Computers were even found in toys, and amidst a whirlwind of mini-, micro-, super-, home-, minisuper- and mainframe computer systems, one could be excused for asking the question: what even is a supercomputer?\n\nTodays Supercomputers\nORNLs Summit supercomputer, fastest until 2020 (Credit: ORNL)\nPerhaps a fair way to classify supercomputers\u00a0 is that the supercomputer aspect is a highly time-limited property. During the 1940s, Colossus and ENIAC were without question the supercomputers of their era, while 1976s Cray-1 wiped the floor with everything that came before, yet all of these are archaic curiosities next to todays top two supercomputers. Both the El Capitan and Frontier supercomputers are exascale level machines  they carry out exaFLOPS in double precision IEEE 754 calculations  based around commodity x86_64 CPUs in a massively parallel configuration.\nTaking up 700 m2 of floor space at the Lawrence Livermore National Laboratory (LLNL) and drawing 30 MW of power, El Capitans 43,808 AMD EPYC CPUs are paired with the same number of AMD Instinct MI300A accelerators, each containing 24 Zen 4 cores plus CDNA3 GPU and 128 GB of HBM3 RAM. Unlike the monolithic ENIAC, El Capitans 11,136 nodes, containing four MI300As each, rely on a number of high-speed interconnects to distribute computing work across all cores.\nAt LLNL, El Capitan is used for effectively the same top secret government things as ENIAC was, while Frontier at Oak Ridge National Laboratory (ORNL) was the fastest supercomputer before El Capitan came online about three years later. Although currently LLNL and ORNL have the fastest supercomputers, there are many more of these systems in use around the world, even for innocent scientific research.\nLooking at the current list of supercomputers, such as todays Top 9, its clear that not only can supercomputers perform a lot more operations per second, they also are invariably massively parallel computing clusters. This wasnt a change that was made easily, as parallel computing comes with a whole stack of complications and problems.\nThe Parallel Computing Shift\nILLIAC IV massively parallel computers Control Unit (CU). (Credit: Steve Jurvetson, Wikimedia)\nThe first massively parallel computer was the ILLIAC IV, conceptualized by Daniel Slotnick in 1952 and first successfully put into operation in 1975 when it was connected to ARPANET. Although only one quadrant was fully constructed, it produced 50 MFLOPS compared to the Cray-1s 160 MFLOPS a year later. Despite the immense construction costs and spotty operational history, it provided a most useful testbed for developing parallel computation methods and algorithms until the system was decommissioned in 1981.\nThere was a lot of pushback against the idea of massively parallel computation, however, with Seymour Cray famously comparing the idea of using many parallel vector processors instead of a single large one akin to plowing a field with 1024 chickens instead of two oxen.\nUltimately there is only so far you can scale a singular vector processor, of course, while parallel computing promised much better scaling, as well as the use of commodity hardware. A good example of this is a so-called Beowulf cluster, named after the original 1994 parallel computer built by Thomas Sterling and Donald Becker at NASA. This can use plain desktop computers, wired together using for example Ethernet and with open source libraries like Open MPI enabling massively parallel computing without a lot of effort.\nNot only does this approach enable the assembly of a supercomputer using cheap-ish, off-the-shelf components, its also effectively the approach used for LLNLs El Capitan, just with not very cheap hardware, and not very cheap interconnect hardware, but still cheaper than if one were to try to build a monolithic vector processor with the same raw processing power after taking the messaging overhead of a cluster into account.\nMini And Maxi\nDavid Lovett of Usagi Electri sitting among his FPS minisupercomputer hardware. (Credit: David Lovett, YouTube)\nOne way to look at supercomputers is that its not about the scale, but what you do with it. Much like how government, large businesses and universities would end up with Big Iron in the form of mainframes and supercomputers, there was a big market for minicomputers too. (At this time mini meant something like a PDP-11 thatd comfortably fit in the corner of an average room at an office or university.)\nThe high-end versions of minicomputers were called superminicomputer, which is not to be confused with minisupercomputer, which is another class entirely. During the 1980s there was a brief surge in this latter class of supercomputers that were designed to bring solid vector computing and similar supercomputer feats down to a size and price tag that might entice departments and other customers whod otherwise not even begin to consider such an investment.\nThe manufacturers of these budget-sized supercomputers were generally not the typical big computer manufacturers, but instead smaller companies and start-ups like Floating Point Systems (later acquired by Cray) who sold array processors and similar parallel, vector computing hardware.\nRecently David Lovett (AKA Mr. Usagi Electric) embarked on a quest to recover and reverse-engineer as much FPS hardware as possible, with one of the goals being to build a full minisupercomputer system as companies and universities might have used them in the 1980s. This would involve attaching such an array processor to a PDP-11/44 system.\nSpeed Versus Reliability\nAmidst all of these definitions, the distinction between a mainframe and a supercomputer is much easier and more straightforward at least. A mainframe is a computer system thats designed for bulk data processing with as much built-in reliability and redundancy as the price tag allows for. A modern example is IBMs Z-series of mainframes, with the Z standing for zero downtime. These kind of systems are used by financial institutions and anywhere else where downtime is counted in millions of dollars going up in (literal) flames every second.\nThis means hot-swappable processor modules, hot-swappable and redundant power supplies, not to mention hot spares and a strong focus on fault tolerant computing. All of these features are less relevant for a supercomputer, where raw performance is the defining factor when running days-long simulations and when other ways to detect flaws exist without requiring hardware-level redundancy.\nConsidering the brief lifespan of supercomputers, currently in the order of a few years, compared to decades with mainframes and the many years that the microcomputers which we have on our desks can last, the life of a supercomputer seems like that of a bright and very brief flame, indeed.\nTop image: Marlyn Wescoff and Betty Jean Jennings configuring plugboards on the ENIAC computer (Source: US National Archives)"}