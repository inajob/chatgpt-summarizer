{"pubDate": "2023-05-31T02:00:07", "original_title": "Simple Cubes Show Off AI-Driven Runtime Changes in VR", "link": "https://hackaday.com/2023/05/30/simple-cubes-show-off-ai-driven-runtime-changes-in-vr/", "source": "https://hackaday.com/blog/feed/", "thumbnail": "https://hackaday.com/wp-content/uploads/2023/05/Cubic-Music-In-Game-footage-1-31-screenshot-1.png", "original_content": "AR and VR developer [Skarredghost] got pretty excited about a virtual blue cube, and for a very good reason. It marked a successful prototype of an augmented reality experience in which the logic underlying the cube as a virtual object was changed by AI in response to verbal direction by the user. Saying make it blue did indeed turn the cube blue! (After a little thinking time, of course.)\nIt didnt stop there, of course, and the blue cube proof-of-concept led to a number of simple demos. The first shows off a row of cubes changing color from red to green in response to musical volume, then a bundle of cubes change size in response to microphone volume, and cubes even start moving around in space.\nThe program accepts spoken input from the user, converts it to text, sends it to a natural language AI model, which then creates the necessary modifications and loads it into the environment to make runtime changes in Unity. The workflow is a bit cumbersome and highlights many of the challenges involved, but it works and thats pretty nifty.\nThe GitHub repository is here and a good demonstration video is embedded just under the page break. Theres also a video with a much more in-depth discussion of whats going on and a frank exploration of the technical challenges.\nIf youre interested in this direction, it seems [Skarredghost] has rounded up the relevant details. And should you have a prototype idea that isnt necessarily AR or VR but would benefit from AI-assisted speech recognition that can run locally? This project has what you need.\n\n"}