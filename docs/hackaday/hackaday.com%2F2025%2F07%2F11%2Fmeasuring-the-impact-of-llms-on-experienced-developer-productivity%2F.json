{"pubDate": "2025-07-12T02:00:10", "original_title": "Measuring the Impact of LLMs on Experienced Developer Productivity", "link": "https://hackaday.com/2025/07/11/measuring-the-impact-of-llms-on-experienced-developer-productivity/", "source": "https://hackaday.com/blog/feed/", "thumbnail": "https://hackaday.com/wp-content/uploads/2025/07/metr_llm_forecasted-vs-observed.png", "original_content": "Recently AI risk and benefit evaluation company METR ran a randomized control test (RCT) on a gaggle of experienced open source developers to gain objective data on how the use of LLMs affects their productivity. Their findings were that using LLM-based tools like Cursor Pro with Claude 3.5/3.7 Sonnet reduced productivity by about 19%, with the full study by [Joel Becker] et al. available as PDF.\nThis study was also intended to establish a methodology to assess the impact from introducing LLM-based tools in software development. In the RCT, 16 experienced open source software developers were given 246 tasks, after which their effective performance was evaluated.\nA large focus of the methodology was on creating realistic scenarios instead of using canned benchmarks. This included adding features to code, bug fixes and refactoring, much as they would do in the work on their respective open source projects. The observed increase in the time it took to complete tasks with the LLMs assistance was found to be likely due to a range of factors, including over-optimism about the LLM tool capabilities, LLMs interfering with existing knowledge on the codebase, poor LLM performance on large codebases, low reliability of the generated code and the LLM doing very poorly on using tactic knowledge and context.\nAlthough METR suggests that this poor showing may improve over time, it seems fair to argue whether LLM coding tools are at all a useful coding partner.", "title": "- LLM\u30c4\u30fc\u30eb\u304c\u30aa\u30fc\u30d7\u30f3\u30bd\u30fc\u30b9\u958b\u767a\u8005\u306e\u751f\u7523\u6027\u306b\u4e0e\u3048\u308b\u5f71\u97ff\u3068\u306f\uff1f", "body": "METR\u306e\u8abf\u67fb\u3067\u3001LLM\u4f7f\u7528\u304c\u958b\u767a\u8005\u306e\u751f\u7523\u6027\u309219%\u4f4e\u4e0b\u3055\u305b\u308b\u7d50\u679c\u306b\u3002", "titles": ["- LLM\u30c4\u30fc\u30eb\u304c\u30aa\u30fc\u30d7\u30f3\u30bd\u30fc\u30b9\u958b\u767a\u8005\u306e\u751f\u7523\u6027\u306b\u4e0e\u3048\u308b\u5f71\u97ff\u3068\u306f\uff1f", "- METR\u306e\u7814\u7a76\u304c\u793a\u3059\u3001LLM\u652f\u63f4\u306b\u3088\u308b\u30bf\u30b9\u30af\u5b8c\u4e86\u6642\u9593\u306e\u5ef6\u3073", "- 19%\u306e\u751f\u7523\u6027\u4f4e\u4e0b\uff1aLLM\u30c4\u30fc\u30eb\u306e\u5b9f\u4f53\u3068\u306f", "- \u30aa\u30fc\u30d7\u30f3\u30bd\u30fc\u30b9\u958b\u767a\u8005\u306b\u304a\u3051\u308bLLM\u30c4\u30fc\u30eb\u306e\u8a55\u4fa1\u7d50\u679c", "- LLM\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u30c4\u30fc\u30eb\u306e\u4fe1\u983c\u6027\u3068\u305d\u306e\u8ab2\u984c"]}