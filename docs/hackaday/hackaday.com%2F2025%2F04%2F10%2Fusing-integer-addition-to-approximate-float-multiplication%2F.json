{"pubDate": "2025-04-11T02:00:28", "original_title": "Using Integer Addition to Approximate Float Multiplication", "link": "https://hackaday.com/2025/04/10/using-integer-addition-to-approximate-float-multiplication/", "source": "https://hackaday.com/blog/feed/", "thumbnail": "https://hackaday.com/wp-content/uploads/2025/04/custom_multiply-3.png", "original_content": "Once the domain of esoteric scientific and business computing, floating point calculations are now practically everywhere. From video games to large language models and kin, it would seem that a processor without floating point capabilities is pretty much a brick at this point. Yet the truth is that integer-based approximations can be good enough to hit the required accuracy. For example, approximating floating point multiplication with integer addition, as [Malte Skarupke] recently had a poke at based on an integer addition-only LLM approach suggested by [Hongyin Luo] and [Wei Sun].\nAs for the way this works, it does pretty much what it says on the tin: adding the two floating point inputs as integer values, followed by adjusting the exponent. This adjustment factor is what gets you close to the answer, but as the article and comments to it illustrate, there are plenty of issues and edge cases you have to concern yourself with. These include under- and overflow, but also specific floating point inputs.\nUnlike in scientific calculations where even minor inaccuracies tend to propagate and cause much larger errors down the line, graphics and LLMs do not care that much about float point precision, so the ~7.5% accuracy of the integer approach is good enough. The question is whether its truly more efficient as the paper suggests, rather than a fallback as seen with e.g. integer-only audio decoders for platforms without an FPU.\nSince one of the nice things about FP-focused vector processors like GPUs and derivatives (tensor, neural, etc.) is that they can churn through a lot of data quite efficiently, the benefits of shifting this to the ALU of a CPU and expecting (energy) improvements seem quite optimistic.", "title": "\u6574\u6570\u6f14\u7b97\u306b\u3088\u308b\u6d6e\u52d5\u5c0f\u6570\u70b9\u8a08\u7b97\u306e\u7c21\u7565\u5316", "body": "\u6d6e\u52d5\u5c0f\u6570\u70b9\u8a08\u7b97\u306f\u591a\u304f\u306e\u5206\u91ce\u3067\u5fc5\u9808\u3060\u304c\u3001\u6574\u6570\u8fd1\u4f3c\u3082\u6709\u52b9\u3002\u7cbe\u5ea6\u306f\u7d047.5%\u3067\u3001\u7279\u5b9a\u306e\u7528\u9014\u306b\u306f\u5341\u5206\u3002", "titles": ["\u6574\u6570\u6f14\u7b97\u306b\u3088\u308b\u6d6e\u52d5\u5c0f\u6570\u70b9\u8a08\u7b97\u306e\u7c21\u7565\u5316", "\u30b2\u30fc\u30e0\u3084AI\u3067\u306e\u6d6e\u52d5\u5c0f\u6570\u70b9\u51e6\u7406\u306e\u4ee3\u66ff\u624b\u6cd5", "\u7cbe\u5ea6\u3068\u52b9\u7387\uff1a\u6574\u6570\u30d9\u30fc\u30b9\u306e\u8a08\u7b97\u306e\u5b9f\u969b", "\u6d6e\u52d5\u5c0f\u6570\u70b9\u306e\u8ab2\u984c\u3068\u6574\u6570\u8fd1\u4f3c\u306e\u9650\u754c", "CPU\u306eALU\u3067\u306e\u30c7\u30fc\u30bf\u51e6\u7406\u306e\u65b0\u305f\u306a\u5c55\u671b"]}