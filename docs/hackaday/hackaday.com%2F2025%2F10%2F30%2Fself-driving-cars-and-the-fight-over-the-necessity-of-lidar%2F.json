{"pubDate": "2025-10-30T14:00:23", "original_title": "Self-Driving Cars and the Fight Over the Necessity of Lidar", "link": "https://hackaday.com/2025/10/30/self-driving-cars-and-the-fight-over-the-necessity-of-lidar/", "source": "https://hackaday.com/blog/feed/", "thumbnail": "https://hackaday.com/wp-content/uploads/2018/03/selfdriving.jpg", "original_content": "If you havent lived underneath a rock for the past decade or so, you will have seen a lot of arguing in the media by prominent figures and their respective fanbases about what the right sensor package is for autonomous vehicles, or self-driving cars in popular parlance. As the task here is to effectively replicate what is achieved by the human Mark 1 eyeball and associated processing hardware in the evolutionary layers of patched-together wetware (human brain), it might seem tempting to think that a bunch of modern RGB cameras and a zippy computer system could do the same vision task quite easily.\nThis is where reality throws a couple of curveballs. Although RGB cameras lack the evolutionary glitches like an inverted image sensor and a big dead spot where the optical nerve punches through said sensor layer, it turns out that the preprocessing performed in the retina, the processing in the visual cortex and analysis in the rest of the brain is really quite good at detecting objects, no doubt helped by millions of years of only those who managed to not get eaten by predators procreating in significant numbers.\nHence the solution of sticking something like a Lidar scanner on a car makes a lot of sense. Not only does this provide advanced details on ones surroundings, but also isnt bothered by rain and fog the way an RGB camera is. Having more and better quality information makes subsequent processing easier and more effective, or so it would seem.\n\nComputer Vision Things\nA Waymo Jaguar I-Pace car in San Francisco. (Credit: Dllu, Wikimedia)\nGiving machines the ability to see and recognize objects has been a dream for many decades, and the subject of nearly an infinite number of science-fiction works. For us humans this ability is developed over the course of our development from a newborn with a still developing visual cortex, to a young adult who by then has hopefully learned how to identify objects in their environment, including details like which objects are edible and which are not.\nAs it turns out, just the first part of that challenge is pretty hard, with interpreting a scene as captured by a camera subject to many possible algorithms that seek to extract edges, infer connections based on various hints as well as the distance to said object and whether its moving or not. All just to answer the basic question of which objects exist in a scene, and what they are currently doing.\nApproaches to object detection can be subdivided into conventional and neural network approaches, with methods employing convolutional neural networks (CNNs) being the most prevalent these days. These CNNs are typically trained with a dataset that is relevant to the objects that will be encountered, such as while navigating in traffic. This is what is used for autonomous cars today by companies like Waymo and Tesla, and is why they need to have both access to a large dataset of traffic videos to train with, as well as a large collection of employees who\u00a0 watch said videos in order to tag as many objects as possible. Once tagged and bundled, these videos then become CNN training data sets.\nThis raises the question of how accurate this approach is. With purely RGB camera images as input, the answer appears to be sorta. Although only considered to be a Class 2 autonomous system according to the SAEs 0-5 rating system, Tesla vehicles with the Autopilot system installed failed to recognize hazards on multiple occasions, including the side of a white truck in 2016, a concrete barrier between a highway and an offramp in 2018, running a red light and rear-ending a fire truck in 2019.\nThis pattern continues year after year, with the Autopilot system failing to recognize hazards and engaging the brakes, including in so-called Full-Self Driving (FSD) mode. In April of 2024, a motorcyclist was run over by a Tesla in FSD mode when the system failed to stop, but instead accelerated. This made it the second fatality involving FSD mode, with the mode now being called FSD Supervised.\nCompared to the considerably less crash-prone Level 4 Waymo cars with their hard to miss sensor packages strapped to the car, one could conceivably make the case that perhaps just a couple of RGB cameras is not enough for reliable object detection, and that quite possibly blending of sensors is a more reliable method for object detection.\nWhich is not to say that Waymo cars are perfect, of course. In 2024 one Waymo car managed to hit a utility pole at low speeds during a pullover maneuver, when the cars firmware incorrectly assessed its response to a situation where a pole-like object was present, but without a hard edge between said pole and the road.\nThis gets us to the second issue with self-driving cars: taking the right decision when confronted with a new situation.\nActing On Perception\nThe Tesla Hardware 4 mainboard with its redundant custom SoCs. (Source: Autopilotreview.com)\nOnce you know what objects are in a scene, and merge this with the known state of the vehicle and, the next step for an autonomous vehicle is to decide what to do with this information. Although the tempting answer might be to also use something with neural networks here, this has turned out to be a non-viable method. Back in 2018 Waymo created a recursive neural network (RNN) called\u00a0ChauffeurNet which was trained on both real-life and synthetic driving data to have it effectively imitate human drivers.\nThe conclusion of this experiment was that while deep learning has a place here, you need to lean mostly on a solid body of rules that provides it with explicit reasoning that copes better with what is called the long tail of possible situations, as you cannot put every conceivable situation in a data set.\nThis thus again turns out to be a place where human input and intelligence are required, as while an RNN or similar can be trained on an impressive data set, it will never be able to learn the reasons for why a decision was made in a training video, nor provide its own reasoning and make reasonable adaptations when faced with a new situation. This is where human experts have to define explicit rules, taking into account the known facts about the current surroundings and state of the vehicle.\nHere is where having details like explicit distance information to an obstacle, its relative speed and dimensions, as well as room to divert to prevent a crash are not just nice to have. Adding sensors like radar and Lidar can provide solid data that an RGB camera plus CNN may also provide if youre lucky, but also maybe not quite. When youre talking about highway speeds and potentially the lives of multiple people at risk, certainty always wins out.\nTesla Hardware And Sneaky Radars\nArbe Phoenix radar module installed in a Tesla car as part of the Hardware 4 Autopilot hardware. (Credit: @greentheonly, Twitter)\nOne of the poorly kept secrets about Teslas Autopilot system is that its had a front-facing radar sensor for most of the time. Starting with Hardware 1 (HW1), it featured a single front-facing camera behind the top of the windshield and a radar behind the lower grille, in addition to 12 ultrasonic sensors around the vehicle.\nNotable is that Tesla did not initially use the radar in a primary object detection role here, meaning that object detection and emergency stop functionality was performed using the RGB cameras. This changed after the RGB camera system failed to notice a white trailer against a bright sky, resulting in a spectacular crash. The subsequent firmware update gave the radar system the same role as the camera system, which likely would have prevented that particular crash.\nHW1 used Mobileyes EyeQ3, but after Mobileye cut ties with Tesla, NVidias Drive PX 2 was used instead for HW2. This upped the number of cameras to eight, providing a surround view of the cars surroundings, with a similar forward-facing radar. After an intermedia HW2.5 revision, HW3 was the first to use a custom processor, featuring twelve Arm Cortex-A72 cores clocked at 2.6 GHz.\nHW3 initially also had a radar sensor, but in 2021 this was eliminated with the Tesla Vision system, which resulted in a significant uptick in crashes. In 2022 it was announced that the ultrasonic sensors for short-range object detection would be removed as well.\nThen in January of 2023 HW4 started shipping, with even more impressive computing specs and 5 MP cameras instead of the previous 1.2 MP ones. This revision also reintroduced the forward-facing radar, apparently the Arbe Phoenix radar with a 300 meter range, but not in the Model Y. This indicates that RGB camera-only perception is still the primary mode for Tesla cars.\nAnswering The Question\nAt this point we can say with a high degree of certainty that by just using RGB cameras it is exceedingly hard to reliably stop a vehicle from smashing into objects, for the simple reason that you are reducing the amount of reliable data that goes into your decision-making software. While the object-detecting CNN may give a 29% possibility of an object being right up ahead, the radar or Lidar will have told you that a big, rather solid-looking object is lying on the road. Your own eyes would have told you that its a large piece of concrete that fell off a truck in front of you.\nThis then mostly leaves the question of whether the front-facing radar thats present in at least some Tesla cars is about as good as the Lidar contraption thats used by other car manufacturers like Volvo, as well as the roof-sized version by Waymo. After all, both work according to roughly the same basic principles.\nThat said, Lidar is superior when it comes to aspects like accuracy, as radar uses longer wavelengths. At the same time a radar system isnt bothered as much by weather conditions, while generally being cheaper. For Waymo the choice for Lidar over radar comes down to this improved detail, as they can create a detailed 3D image of the surroundings, down to the direction that a pedestrian is facing, and hand signals by cyclists.\nThus the shortest possible answer is that yes, Lidar is absolutely the best option, while radar is a pretty good option to at least not drive into that semitrailer and/or pedestrian. Assuming your firmware is properly configured to act on said object detection, natch.", "title": "- \u81ea\u52d5\u904b\u8ee2\u8eca\u306b\u304a\u3051\u308b\u30bb\u30f3\u30b5\u30fc\u306e\u5f79\u5272\u3068\u305d\u306e\u91cd\u8981\u6027", "body": "\u81ea\u52d5\u904b\u8ee2\u8eca\u306e\u30bb\u30f3\u30b5\u30fc\u6280\u8853\u306b\u3064\u3044\u3066\u306e\u8b70\u8ad6\u3092\u7d39\u4ecb\u3057\u3001Lidar\u306e\u91cd\u8981\u6027\u3092\u5f37\u8abf\u3002", "titles": ["- \u81ea\u52d5\u904b\u8ee2\u8eca\u306b\u304a\u3051\u308b\u30bb\u30f3\u30b5\u30fc\u306e\u5f79\u5272\u3068\u305d\u306e\u91cd\u8981\u6027", "- RGB\u30ab\u30e1\u30e9\u3068Lidar: \u81ea\u52d5\u904b\u8ee2\u8eca\u306e\u8996\u899a\u6280\u8853\u306e\u6bd4\u8f03", "- \u81ea\u52d5\u904b\u8ee2\u306b\u304a\u3051\u308b\u5bfe\u8c61\u8a8d\u8b58\u306e\u73fe\u72b6\u3068\u8ab2\u984c", "- \u30c6\u30b9\u30e9\u3068Waymo\u306e\u81ea\u52d5\u904b\u8ee2\u6280\u8853\u306e\u9055\u3044", "- \u74b0\u5883\u306b\u3088\u308b\u5f71\u97ff\u3092\u53d7\u3051\u306a\u3044\u30bb\u30f3\u30b5\u30fc\u306e\u9078\u629e\u3068\u305d\u306e\u30e1\u30ea\u30c3\u30c8"]}