{"pubDate": "2023-08-27T17:00:54", "original_title": "Text Compression Gets Weirdly Efficient With LLMs", "link": "https://hackaday.com/2023/08/27/text-compression-gets-weirdly-efficient-with-llms/", "source": "https://hackaday.com/blog/feed/", "thumbnail": "https://hackaday.com/wp-content/uploads/2023/08/text-of-antique-book-wide.jpg", "original_content": "It used to be that memory and storage space were so precious and so limited of a resource that handling nontrivial amounts of text was a serious problem. Text compression was a highly practical application of computing power.\nToday it might be a solved problem, but that doesnt mean it doesnt attract new or unusual solutions. [Fabrice Bellard] released ts_zip which uses Large Language Models (LLM) to attain text compression ratios higher than any other tool can offer.\nLLMs are the technology behind natural language AIs, and applying them in this way seems effective. The tradeoff? Unlike typical compression tools, the lossless decompression part isnt exactly guaranteed when an LLM is involved. Lossy compression methods are in fact quite useful. JPEG compression, for example, is a good example of discarding data that isnt readily perceived by humans to make a smaller file, but that isnt usually applied to text. If you absolutely require lossless compression, [Fabrice] has that covered with NNCP, a neural-network powered lossless data compressor.\nDo neural networks and LLMs sound far too serious and complicated for your text compression needs? As long as you dont mind a mild amount of definitely noticeable data loss, check out [Greg Kennedy]s Lossy Text Compression which simply, brilliantly, and amusingly uses a thesaurus instead of some fancy algorithms. Yep, it just swaps longer words for shorter ones. Perhaps not the best solution for every need, but between that and [Fabrice]s brilliant work were confident theres something for everyone who craves some novelty with their text compression.\n[Photo by Matthew Henry from Burst]"}