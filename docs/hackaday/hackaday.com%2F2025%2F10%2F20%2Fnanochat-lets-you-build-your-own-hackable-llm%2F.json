{"pubDate": "2025-10-20T11:00:36", "original_title": "Nanochat Lets You Build Your Own Hackable LLM", "link": "https://hackaday.com/2025/10/20/nanochat-lets-you-build-your-own-hackable-llm/", "source": "https://hackaday.com/blog/feed/", "thumbnail": "https://hackaday.com/wp-content/uploads/2025/10/nanochat-banner-e1760743321161.png", "original_content": "Few people know LLMs (Large Language Models) as thoroughly as [Andrej Karpathy], and luckily for us all he expresses that in useful open-source projects. His latest is nanochat, which he bills as a way to create the best ChatGPT $100 can buy.\nWhat is it, exactly? nanochat in a minimal and hackable software project  encapsulated in a single speedrun.sh script  for creating a simple ChatGPT clone from scratch, including web interface. The codebase is about 8,000 lines of clean, readable code with minimal dependencies, making every single part of the process accessible to be tampered with.\nAn accessible, end-to-end codebase for creating a simple ChatGPT clone makes every part of the process hackable.\nThe $100 is the cost of doing the computational grunt work of creating the model, which takes about 4 hours on a single NVIDIA 8XH100 GPU node. The result is a 1.9 billion parameter micro-model, trained on some 38 billion tokens from an open dataset. This model is, as [Andrej] describes in his announcement on X, a little ChatGPT clone you can sort of talk to, and which can write stories/poems, answer simple questions. A walk-through of what that whole process looks like makes it as easy as possible to get started.\nUnsurprisingly, a mere $100 doesnt create a meaningful competitor to modern commercial offerings. However, significant improvements can be had by scaling up the process. A $1,000 version (detailed here) is far more coherent and capable; able to solve simple math or coding problems and take multiple-choice tests.\n[Andrej Karpathy]s work lends itself well to modification and experimentation, and were sure this tool will be no exception. His past work includes a method of training a GPT-2 LLM using only pure C code, and years ago we saw his work on a character-based Recurrent Neural Network (mis)used to generate baroque music by cleverly representing MIDI events as text."}