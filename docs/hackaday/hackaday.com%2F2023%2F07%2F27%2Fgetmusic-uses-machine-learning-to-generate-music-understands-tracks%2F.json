{"pubDate": "2023-07-27T11:00:51", "original_title": "GETMusic Uses Machine Learning to Generate Music, Understands Tracks", "link": "https://hackaday.com/2023/07/27/getmusic-uses-machine-learning-to-generate-music-understands-tracks/", "source": "https://hackaday.com/blog/feed/", "thumbnail": "https://hackaday.com/wp-content/uploads/2023/07/Tutorial-on-using-GETMusic-4-10-screenshot.png", "original_content": "Music generation guided by machine learning can make great projects, but theres not usually much apparent control over the results. The system makes what it makes, and its an achievement if the results are not obvious cacophony. But thats all different with GETMusic which allows for a much more involved approach because it understands and is able to create music by tracks. Among other things, this means one can generate a basic rhythm and melody first, then add additional elements to those existing ones, leaving the previous elements unchanged.\nGETMusic can make music from scratch, or guided from examples, and under the hood uses a diffusion-based approach similar to the method behind AI image generators like Stable Diffusion. Weve previously covered how Stable Diffusion works, but instead of images the same basic principles are used to guide the model from random noise to useful tracks of music.\nJust a few years ago we saw a neural network trained to generate Bach, and while it was capable of moments of brilliance, it didnt produce uniformly-listenable output. GETMusic is on an entirely different level. The model and code are available online and there is a research paper to accompany it.\nYou can watch a video putting it through its paces just below the page break, and there are more videos on the project summary page.\n\n"}