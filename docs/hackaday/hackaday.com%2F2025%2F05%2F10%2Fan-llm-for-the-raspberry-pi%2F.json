{"pubDate": "2025-05-11T02:00:04", "original_title": "An LLM for the Raspberry Pi", "link": "https://hackaday.com/2025/05/10/an-llm-for-the-raspberry-pi/", "source": "https://hackaday.com/blog/feed/", "thumbnail": "https://hackaday.com/wp-content/uploads/2025/05/pillm.png", "youtube": "https://www.youtube.com/watch?v=kbuObvYRnWc", "original_content": "Microsofts latest Phi4 LLM has 14 billion parameters that require about 11 GB of storage. Can you run it on a Raspberry Pi? Get serious. However, the Phi4-mini-reasoning model is a cut-down version with only 3.8 billion parameters that requires 3.2 GB. Thats more realistic and, in a recent video, [Gary Explains] tells you how to add this LLM to your Raspberry Pi arsenal.\nThe version [Gary] uses has four-bit quantization and, as you might expect, the performance isnt going to be stellar. If you are versed in all the LLM lingo, the quantization is the way weights are stored, and, in general, the more parameters a model uses, the more things it can figure out.\n\nAs a benchmark, [Gary] likes to use what he calls the Alice question. In other words, he asks for an answer to this question: Alice has five brothers and she also has three sisters. How many sisters does Alices brother have? While it probably took you a second to think about it, you almost certainly came up with the correct answer. With this model, a Raspberry Pi can answer it, too.\nThe first run seems fairly speedy, but it is running on a PC with a GPU. He notes that the same question takes about 10 minutes to pop up on a Raspberry Pi 5 with 4 cores and 8GB of RAM.\nWe arent sure what youd do with a very slow LLM, but it does work. Let us know what youd use it for, if anything, in the comments.\nThere are some other small models if you dont like Phi4.\n"}