{"pubDate": "2025-01-26T14:00:40", "original_title": "Prompt Injection Tricks AI Into Downloading and Executing Malware", "link": "https://hackaday.com/2025/01/26/prompt-injection-tricks-ai-into-downloading-and-executing-malware/", "source": "https://hackaday.com/blog/feed/", "thumbnail": "https://hackaday.com/wp-content/uploads/2021/08/GithubCopilot.jpg", "original_content": "[wunderwuzzi] demonstrates a proof of concept in which a service that enables an AI to control a virtual computer (in this case, Anthropics Claude Computer Use) is made to download and execute a piece of malware that successfully connects to a command and control (C2) server. [wonderwuzzi] makes the reasonable case that such a system has therefore become a ZombAI. Heres how it worked.\nReferring to the malware as a support tool and embedding instructions into the body of the web page is what got the binary downloaded and executed, compromising the system.\nAfter setting up a web page with a download link to the malicious binary, [wunderwuzzi] attempts to get Claude to download and run the malware. At first, Claude doesnt bite. But that all changes when the content of the HTML page gets rewritten with instructions to download and execute the Support Tool. That new content gets interpreted as orders to follow; being essentially a form of prompt injection.\nClaude dutifully downloads the malicious binary, then autonomously (and cleverly) locates the downloaded file and even uses chmod to make it executable before running it. The result? A compromised machine.\nNow, just to be clear, Claude Computer Use is experimental and this sort of risk is absolutely and explicitly called out in Anthropics documentation. But whats interesting here is that the methods used to convince Claude to compromise the system its using are essentially the same one might take to convince a person. Make something nefarious look innocent, and obfuscate the true source (and intent) of the directions. Watch it in action from beginning to end in a video, embedded just under the page break.\n\nThis is a demonstration of the importance of security and caution when using or designing systems like this. Its also a reminder that large language models (LLMs) fundamentally mix instructions and input data together in the same stream. This is a big part of what makes them so fantastically useful at communicating naturally, but its also why prompt injection is so tricky to truly solve.\n", "title": "- AI\u304c\u30de\u30eb\u30a6\u30a7\u30a2\u3092\u5b9f\u884c\u3059\u308b\u5371\u967a\u6027\uff1a\u5b9f\u8a3c\u5b9f\u9a13\u306e\u8a73\u7d30", "body": "AI\u304c\u30de\u30eb\u30a6\u30a7\u30a2\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3057\u5b9f\u884c\u3059\u308b\u5b9f\u9a13\u304c\u793a\u3055\u308c\u305f\u3002\u30bb\u30ad\u30e5\u30ea\u30c6\u30a3\u306e\u91cd\u8981\u6027\u3092\u518d\u8a8d\u8b58\u3055\u305b\u308b\u5185\u5bb9\u3002", "titles": ["- AI\u304c\u30de\u30eb\u30a6\u30a7\u30a2\u3092\u5b9f\u884c\u3059\u308b\u5371\u967a\u6027\uff1a\u5b9f\u8a3c\u5b9f\u9a13\u306e\u8a73\u7d30", "- \u8105\u5a01\u3092\u5185\u5305\u3059\u308b\u300cZombAI\u300d\uff1aAI\u306b\u3088\u308b\u81ea\u52d5\u5316\u653b\u6483\u306e\u30e1\u30ab\u30cb\u30ba\u30e0", "- \u30d7\u30ed\u30f3\u30d7\u30c8\u30a4\u30f3\u30b8\u30a7\u30af\u30b7\u30e7\u30f3\u306e\u5de7\u5999\u306a\u624b\u53e3\uff1aAI\u304c\u9a19\u3055\u308c\u308b\u77ac\u9593", "- \u30bb\u30ad\u30e5\u30ea\u30c6\u30a3\u306e\u91cd\u8981\u6027\uff1a\u5b9f\u9a13\u7684AI\u306e\u30ea\u30b9\u30af\u3068\u305d\u306e\u5f71\u97ff", "- \u5927\u898f\u6a21\u8a00\u8a9e\u30e2\u30c7\u30eb\uff08LLM\uff09\u306e\u8106\u5f31\u6027\uff1a\u8a2d\u8a08\u3068\u4f7f\u7528\u6642\u306e\u6ce8\u610f\u70b9"]}